AI and Assessment in Higher Education: Results of a Pulse Survey
Meeting Minutes - CoP CTaLE AI in Education Seminar Presenter: Carlos Cortinhas (c.cortinhas@exeter.ac.uk)Institution: University of Exeter
Survey Overview
Survey Period: January 14 - February 28, 2025
Sample Size: 103 valid responses
72% from UK-based academics
28% from academics at non-UK institutions
Survey Structure: 6 multiple choice questions + 3 open-ended questions
Survey Themes Covered
Institutional Context (Q1, Q2, Q3)
Policy and Governance (Q4)
Assessment Adaptation and Practice (Q5, Q6)
Academic Integrity (Q8, Q9)
Equity and Inclusivity (Q7)
Key Findings
Q2: Knowledge of University Policy on Generative AI for Assessment
Key Finding: UK academics were significantly better informed about institutional AI policies than non-UK academics.
Results:
A significant number of academics (especially from non-UK institutions) were unaware of university policies on generative AI
UK institutions showed higher policy awareness rates compared to non-UK institutions
Q3: University Stance on Generative AI in Assessments
Overall Results:
37% - "Allows but does not encourage"
18% - "Allows and encourages"
16% - "Does not allow"
13% - "Depends on instructor/module"
11% - "I don't know"
7% - "No clear policy/ambiguous"
UK vs Non-UK Differences:
Non-UK universities: Higher percentage chose "Does not allow students to use GenAI"
UK universities: Largest category was "Allows but does not encourage"
"Other" responses frequently mentioned "lecturer decides" and "depends on module"
Q4: How Students Are Allowed/Not Allowed to Engage with GenAI
UK Institutions (n≈65):
Disclosure Required: Widely required (statements, prompts, declarations)
Encouragement/Integration: Mixed - some courses actively encourage, others ban
Policy Structure: RAG (Red-Amber-Green) systems or module-led policies
Use for Writing Support: Common (fluency, structure) with declaration required
Copy-Paste of AI Output: Explicitly prohibited in most cases
Training & Support: Provided via library resources or training modules
Monitoring/Enforcement: Mixed enforcement; some use Turnitin AI detector
Non-UK Institutions (n≈30):
Disclosure Required: Common but sometimes informal or vague
Encouragement/Integration: Similar - depends on instructor or module
Policy Structure: Less structured; often instructor discretion
Use for Writing Support: Common (editing, brainstorming), often allowed
Copy-Paste of AI Output: Usually discouraged, sometimes enforced with Turnitin AI
Training & Support: Mentioned but less frequently
Monitoring/Enforcement: Similar, some require retention of AI outputs
Key Takeaways:
UK institutions more likely to adopt structured frameworks (traffic-light systems, AI declarations)
Both UK and non-UK institutions commonly allow AI use for learning with focus on responsible disclosure
Strong consensus on prohibiting unacknowledged AI-generated content
Q5: Assessment Adjustments Made Due to Generative AI Rise
Overall Finding: Most respondents reported changing the type of questions used in assessments
UK vs Non-UK Differences:
UK respondents more likely to adjust: type of questions asked, type of assessments, delivery mode (especially shifting back to in-person)
Non-UK universities showed slightly greater tendency to change assessment format
Significant portion of both groups reported no changes
"Other" responses more frequent among non-UK participants (adding AI appendices, stricter citation, higher expectations)
Q6: How Assessments Have Changed
Statistical Breakdown:
Designing Around AI Weaknesses: 30.26% (largest change in both institution types)
Return to In-Person/Authentic Assessments: 18.42%
Critical Thinking, Analysis and Reflection: 13.16%
Curricular Anchoring: 11.84%
Incorporating AI Critique and Meta-Use: 10.53%
No Assessment Changes: 7.89%
Testing Questions and Expectations: 7.89%
Key Differences:
Non-UK academics: Slightly greater proportion reported designing around AI weaknesses
UK academics: About 20% reported return to in-person or more authentic assessments
Four Main Themes of Assessment Changes
Theme 1: Moving to In-Person Assessment
UK Institution Examples:
Moving assessments away from online tests and take-home coursework
Greater weight given to in-person exams
Shift from online take-home to in-person exams
Non-UK Institution Examples:
Elimination of online exams (pandemic-style)
All exams conducted in-person for skills development (time management)
Converting take-home writing assessments to in-person exams
Theme 2: Creating AI-Resistant Questions
UK Institution Approaches:
Using contemporary cases and specific authors
Students critique AI-generated outputs instead of traditional essays
Greater emphasis on graphs vs mathematical solutions
Focus on economic intuition explanations
Non-UK Institution Approaches:
Questions requiring personal experience and reflections
Connecting to news stories and set course texts
More interactive and locally relevant questions
Graphics-based and complex logical questions
Theme 3: Assessment Format Changes
UK Institution Changes:
Projects instead of essays
Verbal assessments to reduce AI use in written work
Video assessments
Non-UK Institution Changes:
Shorter questions with creative answers instead of long essays
Increased in-person interaction and presentations
Higher weighting for presentations and project/report writing
Theme 4: Maintaining Academic Rigour
UK Institution Strategies:
Enhanced critical thinking requirements in coursework
Questions addressing topical/recent issues where AI tools provide limited answers
Higher standards for achieving top marks
Non-UK Institution Strategies:
More analytical coursework to avoid AI use
Challenging questions difficult for AI to answer fully
Survey Questions Reference
Q1: Is your institution based in the UK? Q2: Do you know what your university policy is on the use of Generative AI for assessment? Q3: Your university policy stance (multiple choice) Q4: How students are allowed/not allowed to engage with generative AI (open-ended) Q5: Assessment adjustments made (select all that apply) Q6: Details on how assessments changed (open-ended) Q7: Considerations of inclusivity and awarding gaps (open-ended) Q8: Are academics allowed to use plagiarism detection tools for AI? (Yes/No/Don't know) Q9: Specific detection methods allowed (multiple choice if Q8 = Yes)

Note: This survey provides valuable insights into how higher education institutions are adapting to the rise of generative AI in academic assessment contexts, highlighting significant differences between UK and non-UK institutional approaches.
Q7: Inclusivity and Awarding Gaps
Key Findings:
Inclusivity and awarding gaps were considered more seriously in UK universities with nearly 32% of them trying to be actively inclusive
UK academics are also much more concerned about how AI will impact inequality
Statistical Breakdown:
Theme
UK Institutions
Non-UK Institutions
% UK (of UK total)
% Non-UK (of Non-UK total)
Explicit consideration of inclusivity
30
5
32%
12%
Concern about AI access inequality
14
1
15%
2%
No/little consideration or unsure
42
24
45%
60%
Rejected relevance or skeptical
7
10
8%
25%
Total responses
93
40
100%
100%

Q7: Inclusivity Consideration Themes (UK Institutions)
Theme
UK Institution Approaches
Equity-aware policy and practice
Tailored support (e.g. extra time, alternative assessments); Some are consciously limiting required AI use to avoid inequality
Concerns about unequal AI access
Acknowledged issues with paid vs free AI tools; Trying to design fair assessments
Assessment format fairness
Preference for exams seen as neutral (e.g., no ethnicity gap); efforts to neutralise advantage
Challenges & uncertainty
Many unsure or unaware of awarding gap implications; some skepticism about question relevance
Proactive inclusive design
Examples include diverse group formation, open learning resources, universal design principles
No or unclear response
Several respondents stated "not applicable", "don't know", or provided no detail

Q8: Plagiarism Detection Tools Use
Key Findings:
In UK universities a far greater proportion of academics are not allowed to use plagiarism detection tools to detect the use of generative AI
However, a surprising high proportion (about 50%) is allowed to use plagiarism detection tools in both UK and non-UK universities
Chart Analysis:
Approximately 50% of both UK and Non-UK institutions allow use of plagiarism detection tools
UK institutions show slightly higher "No" responses compared to Non-UK institutions
Both regions show similar levels of uncertainty ("I do not know")
Q9: If Yes, What Plagiarism Detection Tools?
Key Findings:
A greater proportion of Non-UK universities allow the use of web-based plagiarism detection tools
Running work through GenAI to detect plagiarism is also more widespread outside the UK
The use of web-based plagiarism detection tools is more prevalent in non-UK universities
Detection Methods Breakdown:
Academics are allowed to run student work through ChatGPT or other generative AI to see if it was AI generated (yellow section)
Academics are allowed to hold a viva (oral examination) if they suspect a student of misconduct using generative AI to gather evidence of misconduct (blue section)
Academics are allowed to use web-based plagiarism detection tools (like Turnitin) to detect the use of AI(green section)
Other (orange section)
"Other" category includes:
Academics that do not know what they are able to use
Use of judgement
Other evidence needs to be provided (e.g. fake references)
Q8 Plagiarism Detection & Q5 Assessment Changes
Key Finding: For universities where academics are NOT allowed the use of plagiarism detection tools, there have been significant changes in:
The delivery mode of assessments
The type of questions asked
Analysis: Academics who have not changed any of their assessments (and did not change the delivery mode) tend to come from universities where they are allowed to use web-based plagiarism detection tools.
Chart Categories:
I have not changed any of my assessments (orange)
The type of questions asked (blue)
The type of assessments (green)
The delivery mode of the assessments (moved from online to in-person assessments) (yellow)
Other (dark blue)
Q9 Detection Tools & Q5 Assessment Changes
Key Finding: In universities where academics are allowed to use web-based plagiarism detection tools there has been a significant change in the type of assessments.
Chart Analysis: Shows relationship between different plagiarism detection tool policies and assessment changes across five categories:
I have not changed any of my assessments
The type of questions asked
The type of assessments
The delivery mode of the assessments (moved from online to in-person assessments)
Other
Detection Tool Categories:
Academics are allowed to run the student work through ChatGPT or other generative AI to see if it was AI generated (orange)
Academics are allowed to hold a viva (oral examination) if they suspect a student of misconduct using generative AI to gather evidence of misconduct (blue)
Academics are allowed to use of web-based plagiarism detection tools (like Turnitin) to detect the use of AI(green)
Other (yellow)

Additional Survey Context: This data reveals significant correlations between institutional policies on plagiarism detection tools and the extent to which academics have modified their assessment approaches in response to generative AI. Universities with more restrictive detection policies tend to see greater changes in assessment delivery modes and question types, while those allowing web-based detection tools focus more on changing assessment types rather than delivery methods.





AI in Economics Education: Survey Results Summary
Key Statistics Overview
Sample Size: 94 economics educators
Average AI Adoption Score: 55.8/100 (moderate adoption)
Educators using AI for teaching: 74.7%
Educators encouraging student AI use: 47.7%
Educators who changed assessments: 68.6%
1. First Impressions: What Educators Think of "AI and Economics Education"
Top Word Associations (from 197 total responses):
Opportunity (14 mentions) - Most common positive response
Cheating (11 mentions) - Primary concern
Change (9 mentions) - Recognition of transformation
Challenge (6 mentions) - Acknowledging difficulty
ChatGPT (6 mentions) - Specific tool awareness
Coding (4 mentions) - Technical applications
Sentiment Analysis: Mixed emotions, with educators seeing both opportunities and threats.
2. AI Adoption Levels (Scale 1-100)
Adoption Level
Count
Percentage
Avoid (1-20)
17
18.1%
Low adoption (21-40)
16
17.0%
Moderate (41-60)
15
16.0%
High adoption (61-80)
21
22.3%
Fully embrace (81-100)
25
26.6%

Key Insight: Nearly half (48.9%) of educators are in high adoption or fully embrace categories, while 35.1% remain cautious or resistant.
3. How Educators Use AI for Teaching Preparation
Usage Rate: 74.7% use AI for teaching materials, assessment briefs, or student feedback.
Common Applications:
Content Creation: Slide outlines, problem sets, quiz questions
Language Support: Proofreading, grammar checking (especially for non-native speakers)
Coding Assistance: LaTeX formatting, data visualization, programming examples
Assessment Design: Creating exam questions, solutions, case studies
Literature Support: Finding examples, summarizing papers, bibliographic research
Popular Tools Mentioned: ChatGPT, Mathpix, LaTeX assistance tools
4. Student AI Encouragement
Split Decision: 47.7% encourage student use vs. 52.3% who don't.
When Educators DO Encourage AI Use:
Writing Support: Grammar, proofreading, structure improvement
Coding Assistance: Python, R, Stata programming help
Research Support: Literature surveys, brainstorming, finding data
Concept Exploration: Getting alternative explanations, understanding complex topics
Critical Thinking: Using AI as a thought partner, challenging ideas
Common Restrictions:
Must cite AI use
Cannot replace critical thinking
Should verify AI outputs
Use for enhancement, not replacement
5. Assessment Changes Due to AI
Major Impact: 68.6% of educators have changed their assessment methods.
Most Common Changes:
Moving to In-Person Assessment:
Shift from take-home to in-class exams
More oral presentations and defenses
Increased weight on verbal communication
Changing Assignment Types:
Eliminating traditional essays
Focusing on analysis over summary
Creating "AI-proof" questions requiring deep understanding
More project-based assessments
Raising Standards:
Higher expectations for writing quality (since AI can improve grammar)
More focus on critical thinking and creativity
Emphasis on understanding concepts, not just recall
Quality Control Measures:
Using AI detection software
Requiring reflection on learning process
Students must defend their work in person
More sophisticated questioning that requires genuine understanding
6. Key Challenges Identified
For Educators:
Difficulty distinguishing AI-generated from student work
Frustration with grading AI-assisted essays
Need to redesign entire assessment strategies
Keeping up with rapidly evolving AI capabilities
For Students:
Risk of over-dependence on AI tools
Potential loss of fundamental skills
Academic integrity concerns
Reduced learning from struggle and effort
7. Notable Quotes and Insights
Positive Perspectives:
"ChatGPT makes excellent slide outlines... I work from there"
"I encourage students to use AI for brainstorming ideas, summarizing articles, improving writing clarity"
"Good writing becomes the norm rather than the exception"
Concerns:
"Students use AI for widespread cheating in take-home assignments"
"Marking AI-assisted essays can be extremely frustrating"
"A simple one prompt on an LLM will outperform most students (with no learning)"
Adaptive Strategies:
"I try to make my take-home exam and problem sets AI-proof"
"More focus on creativity of ideas. More focus on verbal communication"
"I design assessments that require reasoning and problem-solving, where relying solely on AI isn't enough"
8. Future Implications
Emerging Trends:
Shift toward competency-based assessment
Increased emphasis on AI literacy as a core skill
Need for institutional policies on AI use
Growing importance of critical evaluation skills
Skills Priority Changes:
Less emphasis on information recall
More focus on analysis and synthesis
Increased value of communication skills
Greater importance of ethical reasoning
Conclusion
The survey reveals a field in transition, with economics educators actively adapting to AI's presence in education. While concerns about academic integrity are significant, there's growing recognition that AI tools, when used appropriately, can enhance both teaching and learning. The key challenge moving forward appears to be developing assessment methods that leverage AI's benefits while ensuring genuine student learning and skill development.
